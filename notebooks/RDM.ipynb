{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "from sisyphus2.tasks import rdm as rd\n",
    "from sisyphus2.backend.models.basic import Basic\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define params globally first before passing to RDM, mess w/ model params and explain what can do. (can turn on and off dale's law). some masking, input output connectivity. train_params variables stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 10 # time-step\n",
    "tau = 100 # takes step of size(dt/tau) times the descent direction when updating state TODO(jasmine): check this\n",
    "T = 2000 # time to run for (number of steps is T/dt)\n",
    "N_batch = 50 # number of trials TODO(jasmine): verify this\n",
    "N_rec = 50 # number of recurrent units TODO(jasmine):verify this\n",
    "name = 'basicModel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm = rd.RDM(dt = dt, tau = tau, T = T, N_batch = N_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDM puts the params passed in as well as other generated params into a dict we can then use to create our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tau': 100, 'N_in': 2, 'N_batch': 50, 'T': 2000, 'N_steps': 200, 'alpha': 0.1, 'dt': 10, 'N_out': 2}\n"
     ]
    }
   ],
   "source": [
    "params = rdm.__dict__\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate *N_batch* trials to be used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = rdm.batch_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add in a few params that Basic(RNN) needs but that RDM doesn't generate for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['name'] = name #Used to scope out a namespace for global variables.\n",
    "params['N_rec'] = N_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some other optional parameters we can add in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['dale_ratio'] = None # Default: None -- when the dale_ratio is set, dale's law is applied\n",
    "params['rec_noise'] = 0.0 # Default: 0.0 -- how much noise to add to the new_state calculation\n",
    "params['W_in_train'] = True # Indicates whether W_in is trainable. Default: True\n",
    "params['W_rec_train'] = True # Indicates whether W_rec is trainable. Default: True\n",
    "params['W_out_train'] = True # Indicates whether W_out is trainable. Default: True\n",
    "params['b_rec_train'] = True # Indicates whether b_rec is trainable. Default: True\n",
    "params['b_out_train'] = True # Indicates whether b_out is trainable. Default: True\n",
    "params['init_state_train'] = True # Indicates whether init_state is trainable. Default: True\n",
    "params['load_weights_path'] = None # When given a path, loads weights from file in that path. Default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicModel = Basic(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a tensorflow session with loss, regularization, predictions, and regularized loss defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicModel.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the training parameters for our model. All of the parameters below are optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-34-71e8e520c2df>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-71e8e520c2df>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    train_params['learning_rate' = .001 # Sets learning rate if use default optimizer Default: .001\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "train_params = {}\n",
    "train_params['save_weights_path'] =  '../weights/refactor_weights.npz' # Where to save the model after training. Default: None\n",
    "train_params['training_iters'] = 1000 # number of iterations to train for Default: 10000\n",
    "train_params['learning_rate' = .001 # Sets learning rate if use default optimizer Default: .001\n",
    "train_params['loss_epoch'] = 10 # Compute and record loss every 'loss_epoch' epochs. Default: 10\n",
    "train_params['verbosity'] = True # If true, prints information as training progresses. Default: True\n",
    "train_params['save_training_weights_epoch'] = 100 # save training weights every 'save_training_weights_epoch' epochs. Default: 100\n",
    "train_params['training_weights_path'] = None # where to save training weights as training progresses. Default: None\n",
    "train_params['generator_function'] = None # replaces trial_batch_generator with the generator_function when not none. Default: None\n",
    "train_params['optimizer'] = tf.train.AdamOptimizer(learning_rate=train_params['learning_rate']) # What optimizer to use to compute gradients. Default: tf.train.AdamOptimizer(learning_rate=train_params['learning_rate'])\n",
    "train_params['clip_grads'] = True # If true, clip gradients by norm 1. Default: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicModel.train(gen, {'save_weights_path': '../weights/refactor_weights.npz', 'training_iters': 500})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the next trial from the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,m = gen.next()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the x value of the trial -- for the RDM, this includes two input neurons ith different coherence. TODO(jasmine): figure out axis scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x[0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the trained model on this trial (not included in the training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(basicModel.test(x)[0][0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean up the model to clear out the tensorflow namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicModel.destruct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
